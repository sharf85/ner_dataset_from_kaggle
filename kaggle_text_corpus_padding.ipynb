{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotated Corpus for Named Entity Recognition (From Kaggle)\n",
    "In this notebook we're building a basic model for NER on the specified corpus using padding up to the most long sentence in the corpus applied to all the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data\n",
    "Downloading the data - it is already well preprocessed - there are a lot of features added to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>lemma</th>\n",
       "      <th>next-lemma</th>\n",
       "      <th>next-next-lemma</th>\n",
       "      <th>next-next-pos</th>\n",
       "      <th>next-next-shape</th>\n",
       "      <th>next-next-word</th>\n",
       "      <th>next-pos</th>\n",
       "      <th>next-shape</th>\n",
       "      <th>next-word</th>\n",
       "      <th>...</th>\n",
       "      <th>prev-prev-lemma</th>\n",
       "      <th>prev-prev-pos</th>\n",
       "      <th>prev-prev-shape</th>\n",
       "      <th>prev-prev-word</th>\n",
       "      <th>prev-shape</th>\n",
       "      <th>prev-word</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>shape</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>281835</th>\n",
       "      <td>0</td>\n",
       "      <td>thousand</td>\n",
       "      <td>of</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>...</td>\n",
       "      <td>__start2__</td>\n",
       "      <td>__START2__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START2__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>1.0</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281836</th>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>...</td>\n",
       "      <td>__start1__</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281837</th>\n",
       "      <td>2</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>have</td>\n",
       "      <td>march</td>\n",
       "      <td>VBN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBP</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>...</td>\n",
       "      <td>thousand</td>\n",
       "      <td>NNS</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281838</th>\n",
       "      <td>3</td>\n",
       "      <td>have</td>\n",
       "      <td>march</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>through</td>\n",
       "      <td>VBN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>...</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281839</th>\n",
       "      <td>4</td>\n",
       "      <td>march</td>\n",
       "      <td>through</td>\n",
       "      <td>london</td>\n",
       "      <td>NNP</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>London</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>through</td>\n",
       "      <td>...</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0     lemma next-lemma next-next-lemma next-next-pos  \\\n",
       "281835           0  thousand         of        demonstr           NNS   \n",
       "281836           1        of   demonstr            have           VBP   \n",
       "281837           2  demonstr       have           march           VBN   \n",
       "281838           3      have      march         through            IN   \n",
       "281839           4     march    through          london           NNP   \n",
       "\n",
       "       next-next-shape next-next-word next-pos next-shape      next-word ...  \\\n",
       "281835       lowercase  demonstrators       IN  lowercase             of ...   \n",
       "281836       lowercase           have      NNS  lowercase  demonstrators ...   \n",
       "281837       lowercase        marched      VBP  lowercase           have ...   \n",
       "281838       lowercase        through      VBN  lowercase        marched ...   \n",
       "281839     capitalized         London       IN  lowercase        through ...   \n",
       "\n",
       "       prev-prev-lemma prev-prev-pos prev-prev-shape prev-prev-word  \\\n",
       "281835      __start2__    __START2__        wildcard     __START2__   \n",
       "281836      __start1__    __START1__        wildcard     __START1__   \n",
       "281837        thousand           NNS     capitalized      Thousands   \n",
       "281838              of            IN       lowercase             of   \n",
       "281839        demonstr           NNS       lowercase  demonstrators   \n",
       "\n",
       "         prev-shape      prev-word sentence_idx        shape           word  \\\n",
       "281835     wildcard     __START1__          1.0  capitalized      Thousands   \n",
       "281836  capitalized      Thousands          1.0    lowercase             of   \n",
       "281837    lowercase             of          1.0    lowercase  demonstrators   \n",
       "281838    lowercase  demonstrators          1.0    lowercase           have   \n",
       "281839    lowercase           have          1.0    lowercase        marched   \n",
       "\n",
       "       tag  \n",
       "281835   O  \n",
       "281836   O  \n",
       "281837   O  \n",
       "281838   O  \n",
       "281839   O  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/ner.csv\", encoding = \"ISO-8859-1\", error_bad_lines=False)\n",
    "df = df.iloc[281835:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to leave only the initial text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>281835</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281836</th>\n",
       "      <td>1.0</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281837</th>\n",
       "      <td>1.0</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281838</th>\n",
       "      <td>1.0</td>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281839</th>\n",
       "      <td>1.0</td>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_idx           word tag\n",
       "281835           1.0      Thousands   O\n",
       "281836           1.0             of   O\n",
       "281837           1.0  demonstrators   O\n",
       "281838           1.0           have   O\n",
       "281839           1.0        marched   O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=df.drop(['Unnamed: 0', 'lemma', 'next-lemma', 'next-next-lemma', 'next-next-pos',\n",
    "       'next-next-shape', 'next-next-word', 'next-pos', 'next-shape',\n",
    "       'next-word', 'prev-iob', 'prev-lemma', 'prev-pos',\n",
    "       'prev-prev-iob', 'prev-prev-lemma', 'prev-prev-pos', 'prev-prev-shape',\n",
    "       'prev-prev-word', 'prev-shape', 'prev-word',\"pos\",\"shape\"],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data into list of sentencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/evgeny/anaconda3/envs/tensorflow_rnn/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/evgeny/anaconda3/envs/tensorflow_rnn/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/evgeny/anaconda3/envs/tensorflow_rnn/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/evgeny/anaconda3/envs/tensorflow_rnn/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/evgeny/anaconda3/envs/tensorflow_rnn/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/evgeny/anaconda3/envs/tensorflow_rnn/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/evgeny/anaconda3/envs/tensorflow_rnn/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/evgeny/anaconda3/envs/tensorflow_rnn/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/evgeny/anaconda3/envs/tensorflow_rnn/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/evgeny/anaconda3/envs/tensorflow_rnn/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/evgeny/anaconda3/envs/tensorflow_rnn/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/evgeny/anaconda3/envs/tensorflow_rnn/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from text_preprocessing import group_sentences, tokenize, pad, split_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forming text sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "Sample label sentence: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n"
     ]
    }
   ],
   "source": [
    "input_sentences, label_sentences = group_sentences(data, \"word\"), group_sentences(data, \"tag\")\n",
    "print(\"Sample sentence:\", input_sentences[0])\n",
    "print(\"Sample label sentence:\", label_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Vocabulary size: 27420\n",
      "y Vocabulary size: 18\n"
     ]
    }
   ],
   "source": [
    "tokenized_input, tokenizer_X = tokenize(input_sentences)\n",
    "tokenized_labels, tokenizer_y = tokenize(label_sentences, to_lower=False)\n",
    "\n",
    "input_vocab_size = len(tokenizer_X.word_index) + 1\n",
    "labels_vocab_size = len(tokenizer_y.word_index) + 1\n",
    "\n",
    "print(\"X Vocabulary size:\", input_vocab_size)\n",
    "print(\"y Vocabulary size:\", labels_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding sentences to the max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_tokenized_input = pad(tokenized_input)\n",
    "padded_tokenized_labels = pad(tokenized_labels)\n",
    "\n",
    "max_sentence_size = padded_tokenized_input.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = padded_tokenized_labels.reshape(*padded_tokenized_labels.shape, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = padded_tokenized_labels.reshape(*padded_tokenized_labels.shape, 1)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_tokenized_input, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import embed_gru_model,embed_bi_gru_model\n",
    "from evaluation import Evaluator\n",
    "eval = Evaluator(tokenizer_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One directional GRU model\n",
    "![RNN](images/embedding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/evgeny/anaconda3/envs/tensorflow_rnn/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/evgeny/anaconda3/envs/tensorflow_rnn/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/evgeny/anaconda3/envs/tensorflow_rnn/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/evgeny/anaconda3/envs/tensorflow_rnn/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:127: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/evgeny/anaconda3/envs/tensorflow_rnn/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3144: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/evgeny/anaconda3/envs/tensorflow_rnn/lib/python3.5/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/evgeny/anaconda3/envs/tensorflow_rnn/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3040: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 81, 128)           3509760   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 81, 128)           98688     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 81, 18)            2322      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 81, 18)            0         \n",
      "=================================================================\n",
      "Total params: 3,610,770\n",
      "Trainable params: 3,610,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_gru_model = embed_gru_model(max_sentence_size, input_vocab_size, labels_vocab_size)\n",
    "embed_gru_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/evgeny/anaconda3/envs/tensorflow_rnn/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 22512 samples, validate on 5629 samples\n",
      "Epoch 1/5\n",
      "22512/22512 [==============================] - 104s 5ms/step - loss: 0.2114 - acc: 0.9556 - val_loss: 0.0618 - val_acc: 0.9845\n",
      "Epoch 2/5\n",
      "22512/22512 [==============================] - 110s 5ms/step - loss: 0.0484 - acc: 0.9865 - val_loss: 0.0466 - val_acc: 0.9865\n",
      "Epoch 3/5\n",
      "22512/22512 [==============================] - 113s 5ms/step - loss: 0.0365 - acc: 0.9888 - val_loss: 0.0437 - val_acc: 0.9871\n",
      "Epoch 4/5\n",
      "22512/22512 [==============================] - 117s 5ms/step - loss: 0.0315 - acc: 0.9899 - val_loss: 0.0426 - val_acc: 0.9875\n",
      "Epoch 5/5\n",
      "22512/22512 [==============================] - 117s 5ms/step - loss: 0.0285 - acc: 0.9906 - val_loss: 0.0429 - val_acc: 0.9873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff6b0155e80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_gru_model.fit(X_train, y_train, batch_size=32, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "      eve       0.42      0.11      0.17        46\n",
      "      org       0.45      0.47      0.46      2930\n",
      "      tim       0.79      0.77      0.78      3007\n",
      "      per       0.68      0.70      0.69      2400\n",
      "      nat       0.67      0.26      0.37        31\n",
      "      art       0.00      0.00      0.00        49\n",
      "      gpe       0.94      0.94      0.94      2337\n",
      "      geo       0.78      0.83      0.80      5485\n",
      "\n",
      "micro avg       0.73      0.74      0.73     16285\n",
      "macro avg       0.72      0.74      0.73     16285\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(eval.evaluate_metrics(y_test, embed_gru_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-directional GRU model\n",
    "![RNN](images/bidirectional.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 81)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 81, 128)           3509760   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 81, 256)           197376    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 81, 18)            4626      \n",
      "=================================================================\n",
      "Total params: 3,711,762\n",
      "Trainable params: 3,711,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_bi_gru_model = embed_bi_gru_model(max_sentence_size, input_vocab_size, labels_vocab_size)\n",
    "embed_bi_gru_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22512 samples, validate on 5629 samples\n",
      "Epoch 1/5\n",
      "22512/22512 [==============================] - 146s 6ms/step - loss: 0.1512 - acc: 0.9660 - val_loss: 0.0473 - val_acc: 0.9867\n",
      "Epoch 2/5\n",
      "22512/22512 [==============================] - 146s 6ms/step - loss: 0.0374 - acc: 0.9889 - val_loss: 0.0383 - val_acc: 0.9887\n",
      "Epoch 3/5\n",
      "22512/22512 [==============================] - 148s 7ms/step - loss: 0.0275 - acc: 0.9915 - val_loss: 0.0369 - val_acc: 0.9892\n",
      "Epoch 4/5\n",
      "22512/22512 [==============================] - 150s 7ms/step - loss: 0.0230 - acc: 0.9927 - val_loss: 0.0372 - val_acc: 0.9892\n",
      "Epoch 5/5\n",
      "22512/22512 [==============================] - 149s 7ms/step - loss: 0.0197 - acc: 0.9936 - val_loss: 0.0391 - val_acc: 0.9890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff69c365b38>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_bi_gru_model.fit(X_train, y_train, batch_size=32, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "      eve       0.45      0.11      0.18        46\n",
      "      org       0.57      0.57      0.57      2930\n",
      "      tim       0.86      0.84      0.85      3007\n",
      "      per       0.71      0.69      0.70      2400\n",
      "      nat       0.73      0.26      0.38        31\n",
      "      art       0.00      0.00      0.00        49\n",
      "      gpe       0.95      0.93      0.94      2337\n",
      "      geo       0.84      0.84      0.84      5485\n",
      "\n",
      "micro avg       0.79      0.78      0.78     16285\n",
      "macro avg       0.79      0.78      0.78     16285\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(eval.evaluate_metrics(y_test, embed_bi_gru_model.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
